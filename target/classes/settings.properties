# Azure Blob Settings
azureBlobConnectionString=DefaultEndpointsProtocol=https;AccountName=testcjdatafactory;AccountKey=qYaZm3m8+Z2aYAiSDzzvStkTUgZXl29U76lDJ0qiob7bbV4g7kjtwO+FI2QoptGdZgEdtsAYzG8T0hl5TeftWA==;EndpointSuffix=core.windows.net

# Azure Data Lake Settings
adlAccountFQDN=enter-adlAccountFQDN
adlClientId=enter-adlClientId
adlAuthTokenEndpoint=enter-adlAuthTokenEndpoint
adlClientKey=enter-adlClientKey
adlSourceFolder=/backcompattest/
adlShuffleFolder=/shuffleTest/
adlSortedDataFolder=/sortedDummyData/

# CosmosDB Settings
cosmosDbEndPoint=https://testbulkimport.documents.azure.com:443/
cosmosDbMasterkey=Jn5gpEE0TcmFLVwQ9avcUNaRZDSktkvKgeRVMmRQy97RILzd9Ce9y6FYec9P3IkyBUD8K11zVYnCf4J9zq65Yg==
cosmosDbDatabase=BulkImportTestDatabase
cosmosDbImportTrackingCollection=partitionstracking
importTrackingCollectionThroughput=1000
cosmosDbDataCollection=TestColl
cosmosDbDataCollectionPkValue=region
cosmosDbDataCollectionThroughput=1000000
cosmosDbImportMiniBatchMaxSizeInBytes=-1

# VM with more cores and Memory should increase this number
# 16 CPUs and 32GB VM can set 6000
cosmosDbDataCollectionConnectionPoolSize=60

# Based on the RUs provisioned and document size please tune this number
# until you see better RU utilization
cosmosDbBulkImportLibBatchSize=-1

# Settings for mapping keys between input json data and cosmos db document
# Following settings creates Cosmos DB Document where Id=RandomGUID and PartitionKey= 'Key' column in Input Json
idField=
pkField=region
useGuidForId=true
useGuidForPk=false

# Scale test settings
# Make sure ingestion mode in command line as 'scaletest'
runTag=5BatchRun
preCookedDataQueueSize=5

# Ingestion worker settings
partitionsLimitForWorker=135
jsonDocsBatchCount=20000

mergePartitions=true
dummyFilesCount=656

# upload one dummy file in ADL and provide the path
dummyFilePath=/source/part-v000-0000-r-00000

# USQL settings
adlaAccountName=adobeusql
adlaTenantId=enter-adlaTenantId
adlaSubId=enter-adlaSubId
adlaClientId=enter-adlaClientId
adlaClientSecret=enter-adlaClientSecret
shuffleUsqlScriptOutputFolder=shuffle
sortUsqlScriptOutputFolder=sort

# Json payload column in CSV
# For debugging purpose script emits rownumber and hashkey along with Json Doc
jsonDocColumnIndexInCsv=1

# By default USql allows to submit only 200 jobs
# so 200 is better
numberOfUsqlJobs=200

# Applies only to shuffle
# All files source folders sorted by name in ascending order
# and chooses number of files to be processed
numberOfSourceDataFilesToProcess=30000

numberOfCosmosDbPartitions=10

# Keep big number so it will order the files based on the max limit
estimatedNumberOfFilesInPartition=100