# Azure Blob Settings
azureBlobConnectionString=enter-azureBlobConnectionString

# Azure Data Lake Settings
adlAccountFQDN=enter-adlAccountFQDN
adlClientId=enter-adlClientId
adlAuthTokenEndpoint=enter-adlAuthTokenEndpoint
adlClientKey=enter-adlClientKey
adlSourceFolder=/backcompattest/
adlShuffleFolder=/shuffleTest/
adlSortedDataFolder=/sortedDummyData/

# CosmosDB Settings
cosmosDbEndPoint=enter-cosmosDbEndPoint
cosmosDbMasterkey=enter-cosmosDbMasterkey
cosmosDbDatabase= SortPerf
cosmosDbImportTrackingCollection=partitionstracking6
importTrackingCollectionThroughput=10000
cosmosDbDataCollection=Coll3
cosmosDbDataCollectionPkValue=key
cosmosDbDataCollectionThroughput=100000
cosmosDbImportMiniBatchMaxSizeInBytes=-1

# VM with more cores and Memory should increase this number
# 16 CPUs and 32GB VM can set 6000
cosmosDbDataCollectionConnectionPoolSize=6000

# Based on the RUs provisioned and document size please tune this number
# until you see better RU utilization
cosmosDbBulkImportLibBatchSize=-1

# Settings for mapping keys between input json data and cosmos db document
# Following settings creates Cosmos DB Document where Id=RandomGUID and PartitionKey= 'Key' column in Input Json
idField=id
useGuidForId=true
useGuidForPk=true

# Scale test settings
# Make sure ingestion mode in command line as 'scaletest'
runTag=5BatchRun
preCookedDataQueueSize=5

# Ingestion worker settings
partitionsLimitForWorker=135
jsonDocsBatchCount=20000

mergePartitions=true
dummyFilesCount=656

# upload one dummy file in ADL and provide the path
dummyFilePath=/source/part-v000-0000-r-00000

# USQL settings
adlaAccountName=adobeusql
adlaTenantId=enter-adlaTenantId
adlaSubId=enter-adlaSubId
adlaClientId=enter-adlaClientId
adlaClientSecret=enter-adlaClientSecret
shuffleUsqlScriptOutputFolder=shuffle
sortUsqlScriptOutputFolder=sort

# Json payload column in CSV
# For debugging purpose script emits rownumber and hashkey along with Json Doc
jsonDocColumnIndexInCsv=1

# By default USql allows to submit only 200 jobs
# so 200 is better
numberOfUsqlJobs=200

# Applies only to shuffle
# All files source folders sorted by name in ascending order
# and chooses number of files to be processed
numberOfSourceDataFilesToProcess=30000

numberOfCosmosDbPartitions=10

# Keep big number so it will order the files based on the max limit
estimatedNumberOfFilesInPartition=100